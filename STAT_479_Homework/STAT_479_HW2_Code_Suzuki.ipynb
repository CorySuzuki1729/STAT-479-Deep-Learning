{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 479 Homework 2\n",
    "Cory Suzuki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data and check the shape of the dataset. \n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "cancer_data.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a. There are 569 samples in this dataset and 30 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dataset into a Pandas dataframe and find the mean concavity for observation 564. \n",
    "\n",
    "cancer_new = pd.DataFrame(data=cancer_data.data, columns=cancer_data.feature_names)\n",
    "cancer_new['target'] = cancer_data.target\n",
    "cancer_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b. The mean concavity for observation 564 is reported to be 0.24390."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30)\n",
      "(455, 1)\n",
      "(57, 30)\n",
      "(57, 1)\n",
      "(57, 30)\n",
      "(57, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split the original dataset into training, validation, and testing sets with an 80-10-10 split. \n",
    "\n",
    "X = cancer_new.drop(['target'], axis=1)\n",
    "y = cancer_new.drop(['mean radius', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity',\n",
    "                      'mean concave points', 'mean symmetry', 'mean fractal dimension', 'mean texture', 'radius error', 'texture error',\n",
    "                        'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error',\n",
    "                          'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter',\n",
    "                     'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry',\n",
    "                       'worst fractal dimension'], axis=1)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=.20, shuffle=True, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=.50, shuffle=True, random_state=42)\n",
    "\n",
    "splits = [X_train, y_train, X_valid, y_valid, X_test, y_test]\n",
    "for index in splits:\n",
    "    print(index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the training, validation, and testing sets using the StandardScaler() from Sci-Kit Learn's Preprocessing module. \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(X_train)\n",
    "scaler.transform(X_valid)\n",
    "scaler.transform(X_test)\n",
    "sample_size_cancer, dimensions_cancer = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedforward Neural Network (FNN) with one hidden layer with 16 neurons and ReLU activation and an output layer with 1\n",
    "# neuron and sigmoid activation for binary classification. \n",
    "\n",
    "keras_FNN_cancer = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(dimensions_cancer,)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using binary cross entropy loss function, Adam optimizer, and tracking accuracy during training. \n",
    "\n",
    "keras_FNN_cancer.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6163 - loss: 424.4063 - val_accuracy: 0.5263 - val_loss: 467.9424\n",
      "Epoch 2/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6343 - loss: 340.1393 - val_accuracy: 0.5263 - val_loss: 380.8787\n",
      "Epoch 3/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6334 - loss: 289.6225 - val_accuracy: 0.5263 - val_loss: 298.1970\n",
      "Epoch 4/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6190 - loss: 231.7980 - val_accuracy: 0.5263 - val_loss: 223.3968\n",
      "Epoch 5/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6136 - loss: 166.4760 - val_accuracy: 0.5263 - val_loss: 152.6719\n",
      "Epoch 6/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6235 - loss: 109.6467 - val_accuracy: 0.5263 - val_loss: 83.8613\n",
      "Epoch 7/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6335 - loss: 44.9411 - val_accuracy: 0.2982 - val_loss: 18.6434\n",
      "Epoch 8/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3522 - loss: 12.5718 - val_accuracy: 0.4737 - val_loss: 12.3789\n",
      "Epoch 9/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2994 - loss: 12.1565 - val_accuracy: 0.2105 - val_loss: 10.9145\n",
      "Epoch 10/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3330 - loss: 9.1787 - val_accuracy: 0.2982 - val_loss: 7.9209\n",
      "Epoch 11/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2803 - loss: 8.1086 - val_accuracy: 0.2807 - val_loss: 7.0542\n",
      "Epoch 12/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3103 - loss: 6.8787 - val_accuracy: 0.2982 - val_loss: 5.5870\n",
      "Epoch 13/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3196 - loss: 6.1334 - val_accuracy: 0.4035 - val_loss: 4.1620\n",
      "Epoch 14/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4342 - loss: 4.4245 - val_accuracy: 0.5088 - val_loss: 2.4447\n",
      "Epoch 15/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5061 - loss: 2.8048 - val_accuracy: 0.6842 - val_loss: 1.6556\n",
      "Epoch 16/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6170 - loss: 1.7887 - val_accuracy: 0.8246 - val_loss: 1.2071\n",
      "Epoch 17/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6992 - loss: 1.4452 - val_accuracy: 0.8246 - val_loss: 1.0232\n",
      "Epoch 18/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7309 - loss: 1.1682 - val_accuracy: 0.8421 - val_loss: 0.8895\n",
      "Epoch 19/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7430 - loss: 1.0474 - val_accuracy: 0.8421 - val_loss: 0.7920\n",
      "Epoch 20/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7811 - loss: 0.8586 - val_accuracy: 0.8772 - val_loss: 0.7299\n",
      "Epoch 21/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7812 - loss: 0.8502 - val_accuracy: 0.8596 - val_loss: 0.6525\n",
      "Epoch 22/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7993 - loss: 0.8052 - val_accuracy: 0.8596 - val_loss: 0.6046\n",
      "Epoch 23/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7935 - loss: 0.7273 - val_accuracy: 0.8772 - val_loss: 0.5643\n",
      "Epoch 24/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8158 - loss: 0.6401 - val_accuracy: 0.8772 - val_loss: 0.5179\n",
      "Epoch 25/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8136 - loss: 0.8013 - val_accuracy: 0.8772 - val_loss: 0.5440\n",
      "Epoch 26/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8121 - loss: 0.5762 - val_accuracy: 0.8596 - val_loss: 0.4826\n",
      "Epoch 27/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8589 - loss: 0.5740 - val_accuracy: 0.8772 - val_loss: 0.4212\n",
      "Epoch 28/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8639 - loss: 0.5898 - val_accuracy: 0.8772 - val_loss: 0.3946\n",
      "Epoch 29/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8418 - loss: 0.4809 - val_accuracy: 0.8947 - val_loss: 0.4054\n",
      "Epoch 30/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8745 - loss: 0.4730 - val_accuracy: 0.8947 - val_loss: 0.3796\n",
      "Epoch 31/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8835 - loss: 0.4995 - val_accuracy: 0.8947 - val_loss: 0.3362\n",
      "Epoch 32/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8758 - loss: 0.4032 - val_accuracy: 0.8947 - val_loss: 0.3221\n",
      "Epoch 33/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8743 - loss: 0.4126 - val_accuracy: 0.9123 - val_loss: 0.3067\n",
      "Epoch 34/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8613 - loss: 0.4855 - val_accuracy: 0.9123 - val_loss: 0.2915\n",
      "Epoch 35/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9013 - loss: 0.3085 - val_accuracy: 0.9123 - val_loss: 0.2861\n",
      "Epoch 36/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9021 - loss: 0.3356 - val_accuracy: 0.9123 - val_loss: 0.2802\n",
      "Epoch 37/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8903 - loss: 0.3945 - val_accuracy: 0.9123 - val_loss: 0.2540\n",
      "Epoch 38/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9104 - loss: 0.3254 - val_accuracy: 0.8947 - val_loss: 0.2410\n",
      "Epoch 39/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8877 - loss: 0.3619 - val_accuracy: 0.8772 - val_loss: 0.4362\n",
      "Epoch 40/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8704 - loss: 0.5581 - val_accuracy: 0.8947 - val_loss: 0.2253\n",
      "Epoch 41/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8802 - loss: 0.4374 - val_accuracy: 0.8947 - val_loss: 0.2080\n",
      "Epoch 42/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8562 - loss: 0.4378 - val_accuracy: 0.9123 - val_loss: 0.3483\n",
      "Epoch 43/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8869 - loss: 0.4058 - val_accuracy: 0.9298 - val_loss: 0.2027\n",
      "Epoch 44/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9086 - loss: 0.3394 - val_accuracy: 0.9123 - val_loss: 0.1865\n",
      "Epoch 45/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9020 - loss: 0.3265 - val_accuracy: 0.9123 - val_loss: 0.2550\n",
      "Epoch 46/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8859 - loss: 0.3673 - val_accuracy: 0.9474 - val_loss: 0.1930\n",
      "Epoch 47/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8876 - loss: 0.3554 - val_accuracy: 0.9123 - val_loss: 0.1936\n",
      "Epoch 48/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8839 - loss: 0.3852 - val_accuracy: 0.9123 - val_loss: 0.1595\n",
      "Epoch 49/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8720 - loss: 0.4453 - val_accuracy: 0.9123 - val_loss: 0.2801\n",
      "Epoch 50/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9143 - loss: 0.2872 - val_accuracy: 0.9123 - val_loss: 0.1590\n",
      "Epoch 51/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8942 - loss: 0.3505 - val_accuracy: 0.9474 - val_loss: 0.1508\n",
      "Epoch 52/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8941 - loss: 0.3451 - val_accuracy: 0.9474 - val_loss: 0.1417\n",
      "Epoch 53/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9113 - loss: 0.2502 - val_accuracy: 0.8947 - val_loss: 0.3510\n",
      "Epoch 54/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8999 - loss: 0.3592 - val_accuracy: 0.9298 - val_loss: 0.1292\n",
      "Epoch 55/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9111 - loss: 0.3025 - val_accuracy: 0.9298 - val_loss: 0.1230\n",
      "Epoch 56/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9129 - loss: 0.3155 - val_accuracy: 0.9123 - val_loss: 0.3017\n",
      "Epoch 57/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8890 - loss: 0.3575 - val_accuracy: 0.9298 - val_loss: 0.2125\n",
      "Epoch 58/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8984 - loss: 0.3144 - val_accuracy: 0.9298 - val_loss: 0.1144\n",
      "Epoch 59/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9330 - loss: 0.2422 - val_accuracy: 0.9474 - val_loss: 0.1147\n",
      "Epoch 60/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9085 - loss: 0.2563 - val_accuracy: 0.9298 - val_loss: 0.1999\n",
      "Epoch 61/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8935 - loss: 0.3199 - val_accuracy: 0.9474 - val_loss: 0.1402\n",
      "Epoch 62/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9053 - loss: 0.3038 - val_accuracy: 0.9298 - val_loss: 0.2137\n",
      "Epoch 63/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9218 - loss: 0.2769 - val_accuracy: 0.9474 - val_loss: 0.1157\n",
      "Epoch 64/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9218 - loss: 0.2390 - val_accuracy: 0.9649 - val_loss: 0.1130\n",
      "Epoch 65/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9192 - loss: 0.2637 - val_accuracy: 0.9298 - val_loss: 0.1789\n",
      "Epoch 66/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9310 - loss: 0.2651 - val_accuracy: 0.9474 - val_loss: 0.1256\n",
      "Epoch 67/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9222 - loss: 0.2157 - val_accuracy: 0.8947 - val_loss: 0.2999\n",
      "Epoch 68/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8992 - loss: 0.3102 - val_accuracy: 0.9474 - val_loss: 0.1281\n",
      "Epoch 69/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9259 - loss: 0.2172 - val_accuracy: 0.9649 - val_loss: 0.1070\n",
      "Epoch 70/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9244 - loss: 0.2324 - val_accuracy: 0.9474 - val_loss: 0.0923\n",
      "Epoch 71/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9195 - loss: 0.2154 - val_accuracy: 0.9123 - val_loss: 0.2269\n",
      "Epoch 72/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8864 - loss: 0.2767 - val_accuracy: 0.9298 - val_loss: 0.1645\n",
      "Epoch 73/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9374 - loss: 0.1714 - val_accuracy: 0.9649 - val_loss: 0.0984\n",
      "Epoch 74/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9087 - loss: 0.2125 - val_accuracy: 0.9298 - val_loss: 0.1912\n",
      "Epoch 75/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9272 - loss: 0.2599 - val_accuracy: 0.9649 - val_loss: 0.1029\n",
      "Epoch 76/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9032 - loss: 0.2716 - val_accuracy: 0.9649 - val_loss: 0.0904\n",
      "Epoch 77/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9262 - loss: 0.1933 - val_accuracy: 0.9123 - val_loss: 0.2054\n",
      "Epoch 78/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9106 - loss: 0.2670 - val_accuracy: 0.9123 - val_loss: 0.2171\n",
      "Epoch 79/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8919 - loss: 0.3058 - val_accuracy: 0.9649 - val_loss: 0.0854\n",
      "Epoch 80/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9284 - loss: 0.1807 - val_accuracy: 0.9298 - val_loss: 0.1429\n",
      "Epoch 81/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9296 - loss: 0.1716 - val_accuracy: 0.9649 - val_loss: 0.1029\n",
      "Epoch 82/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9416 - loss: 0.1585 - val_accuracy: 0.9649 - val_loss: 0.0881\n",
      "Epoch 83/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9496 - loss: 0.1522 - val_accuracy: 0.9649 - val_loss: 0.0921\n",
      "Epoch 84/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9269 - loss: 0.1717 - val_accuracy: 0.8947 - val_loss: 0.2041\n",
      "Epoch 85/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9398 - loss: 0.1797 - val_accuracy: 0.9298 - val_loss: 0.1509\n",
      "Epoch 86/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9212 - loss: 0.2626 - val_accuracy: 0.9649 - val_loss: 0.0872\n",
      "Epoch 87/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9194 - loss: 0.1961 - val_accuracy: 0.9649 - val_loss: 0.0875\n",
      "Epoch 88/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9279 - loss: 0.1919 - val_accuracy: 0.9649 - val_loss: 0.0883\n",
      "Epoch 89/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9163 - loss: 0.2217 - val_accuracy: 0.9298 - val_loss: 0.1221\n",
      "Epoch 90/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9346 - loss: 0.1750 - val_accuracy: 0.9649 - val_loss: 0.0933\n",
      "Epoch 91/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8994 - loss: 0.1928 - val_accuracy: 0.9298 - val_loss: 0.1261\n",
      "Epoch 92/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9134 - loss: 0.1911 - val_accuracy: 0.9298 - val_loss: 0.1376\n",
      "Epoch 93/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9257 - loss: 0.1868 - val_accuracy: 0.9298 - val_loss: 0.1055\n",
      "Epoch 94/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9070 - loss: 0.2366 - val_accuracy: 0.9298 - val_loss: 0.1032\n",
      "Epoch 95/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9093 - loss: 0.2017 - val_accuracy: 0.8947 - val_loss: 0.2748\n",
      "Epoch 96/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9396 - loss: 0.1612 - val_accuracy: 0.9474 - val_loss: 0.0955\n",
      "Epoch 97/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9431 - loss: 0.1479 - val_accuracy: 0.9298 - val_loss: 0.1082\n",
      "Epoch 98/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9373 - loss: 0.1671 - val_accuracy: 0.9298 - val_loss: 0.1275\n",
      "Epoch 99/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9131 - loss: 0.1996 - val_accuracy: 0.9298 - val_loss: 0.1359\n",
      "Epoch 100/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9336 - loss: 0.1701 - val_accuracy: 0.9123 - val_loss: 0.1586\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9558 - loss: 0.1207 \n",
      "Train score: [0.16356204450130463, 0.9340659379959106]\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9103 - loss: 0.1645  \n",
      "Validation score: [0.15858685970306396, 0.9122806787490845]\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 100 epochs, but include the validation data during training by passing the validation_data argument to \n",
    "# model.fit(). Use the training data for training and validation data for monitoring overfitting. Record the loss and accuracy\n",
    "# for both training and validation data at each epoch. \n",
    "\n",
    "train_FNN_cancer = keras_FNN_cancer.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=100)\n",
    "print(\"Train score:\", keras_FNN_cancer.evaluate(X_train, y_train))\n",
    "print(\"Validation score:\", keras_FNN_cancer.evaluate(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# After training, plot the training loss and validation loss over the 100 epochs. Use this plot to visually determine the point\n",
    "# at which the model begins to overfit (when the validation loss stops decreasing and starts to increase) \n",
    "\n",
    "plt.plot(train_FNN_cancer.history['loss'], label='training_loss')\n",
    "plt.plot(train_FNN_cancer.history['val_loss'], label='validation_loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9354 - loss: 0.1591 - val_accuracy: 1.0000 - val_loss: 0.0546\n",
      "Epoch 2/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9237 - loss: 0.1544 - val_accuracy: 0.9825 - val_loss: 0.0395\n",
      "Epoch 3/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9213 - loss: 0.2018 - val_accuracy: 0.9825 - val_loss: 0.0559\n",
      "Epoch 4/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9375 - loss: 0.1449 - val_accuracy: 0.9298 - val_loss: 0.1392\n",
      "Epoch 5/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9175 - loss: 0.2384 - val_accuracy: 1.0000 - val_loss: 0.0476\n",
      "Epoch 6/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9330 - loss: 0.1561 - val_accuracy: 1.0000 - val_loss: 0.0343\n",
      "Epoch 7/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9191 - loss: 0.1753 - val_accuracy: 1.0000 - val_loss: 0.0384\n",
      "Epoch 8/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9357 - loss: 0.1545 - val_accuracy: 0.9825 - val_loss: 0.0661\n",
      "Epoch 9/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9205 - loss: 0.1647 - val_accuracy: 1.0000 - val_loss: 0.0380\n",
      "Epoch 10/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9260 - loss: 0.1835 - val_accuracy: 1.0000 - val_loss: 0.0371\n",
      "Epoch 11/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9252 - loss: 0.1936 - val_accuracy: 1.0000 - val_loss: 0.0373\n",
      "Epoch 12/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9347 - loss: 0.1695 - val_accuracy: 0.9825 - val_loss: 0.0705\n",
      "Epoch 13/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9524 - loss: 0.1262 - val_accuracy: 0.9825 - val_loss: 0.0598\n",
      "Epoch 14/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9208 - loss: 0.1709 - val_accuracy: 0.9649 - val_loss: 0.0810\n",
      "Epoch 15/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9223 - loss: 0.1877 - val_accuracy: 0.9825 - val_loss: 0.0663\n",
      "Epoch 16/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9420 - loss: 0.1425 - val_accuracy: 0.9825 - val_loss: 0.0535\n",
      "Epoch 17/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9311 - loss: 0.1743 - val_accuracy: 1.0000 - val_loss: 0.0499\n",
      "Epoch 18/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9374 - loss: 0.1535 - val_accuracy: 0.9474 - val_loss: 0.1029\n",
      "Epoch 19/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9384 - loss: 0.1716 - val_accuracy: 0.9298 - val_loss: 0.1514\n",
      "Epoch 20/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9317 - loss: 0.1948 - val_accuracy: 0.9825 - val_loss: 0.0811\n",
      "Epoch 21/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9254 - loss: 0.1615 - val_accuracy: 1.0000 - val_loss: 0.0453\n",
      "Epoch 22/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9268 - loss: 0.1467 - val_accuracy: 0.9649 - val_loss: 0.0955\n",
      "Epoch 23/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9440 - loss: 0.1381 - val_accuracy: 1.0000 - val_loss: 0.0428\n",
      "Epoch 24/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9254 - loss: 0.2051 - val_accuracy: 0.9298 - val_loss: 0.1337\n",
      "Epoch 25/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9165 - loss: 0.2457 - val_accuracy: 0.9825 - val_loss: 0.0536\n",
      "Epoch 26/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9015 - loss: 0.2119 - val_accuracy: 0.9825 - val_loss: 0.0420\n",
      "Epoch 27/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9533 - loss: 0.1250 - val_accuracy: 0.9825 - val_loss: 0.0661\n",
      "Epoch 28/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9251 - loss: 0.1566 - val_accuracy: 1.0000 - val_loss: 0.0447\n",
      "Epoch 29/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9380 - loss: 0.1238 - val_accuracy: 0.9825 - val_loss: 0.0422\n",
      "Epoch 30/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9362 - loss: 0.1524 - val_accuracy: 0.9825 - val_loss: 0.0519\n",
      "Epoch 31/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9353 - loss: 0.1650 - val_accuracy: 0.9649 - val_loss: 0.0962\n",
      "Epoch 32/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9453 - loss: 0.1291 - val_accuracy: 0.9825 - val_loss: 0.0681\n",
      "Epoch 33/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9513 - loss: 0.1271 - val_accuracy: 0.9825 - val_loss: 0.0565\n",
      "Epoch 34/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9455 - loss: 0.1370 - val_accuracy: 0.9825 - val_loss: 0.0548\n",
      "Epoch 35/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9371 - loss: 0.1352 - val_accuracy: 1.0000 - val_loss: 0.0514\n",
      "Epoch 36/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9331 - loss: 0.1352 - val_accuracy: 0.9825 - val_loss: 0.0447\n",
      "Epoch 37/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9499 - loss: 0.1410 - val_accuracy: 0.9825 - val_loss: 0.0446\n",
      "Epoch 38/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9446 - loss: 0.1263 - val_accuracy: 0.9825 - val_loss: 0.0604\n",
      "Epoch 39/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9491 - loss: 0.1246 - val_accuracy: 0.9825 - val_loss: 0.0787\n",
      "Epoch 40/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9465 - loss: 0.1399 - val_accuracy: 0.9825 - val_loss: 0.0555\n",
      "Epoch 41/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9591 - loss: 0.1217 - val_accuracy: 0.9825 - val_loss: 0.0589\n",
      "Epoch 42/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9452 - loss: 0.1295 - val_accuracy: 0.9825 - val_loss: 0.0650\n",
      "Epoch 43/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9328 - loss: 0.1591 - val_accuracy: 0.9825 - val_loss: 0.0810\n",
      "Epoch 44/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9404 - loss: 0.1422 - val_accuracy: 0.9825 - val_loss: 0.0475\n",
      "Epoch 45/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9431 - loss: 0.1328 - val_accuracy: 0.9825 - val_loss: 0.0587\n",
      "Epoch 46/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9671 - loss: 0.0963 - val_accuracy: 0.9825 - val_loss: 0.0757\n",
      "Epoch 47/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9372 - loss: 0.1490 - val_accuracy: 1.0000 - val_loss: 0.0506\n",
      "Epoch 48/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9454 - loss: 0.1367 - val_accuracy: 0.9825 - val_loss: 0.0463\n",
      "Epoch 49/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9432 - loss: 0.1314 - val_accuracy: 1.0000 - val_loss: 0.0502\n",
      "Epoch 50/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9550 - loss: 0.1233 - val_accuracy: 0.9825 - val_loss: 0.0507\n",
      "Epoch 51/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9307 - loss: 0.1637 - val_accuracy: 0.9825 - val_loss: 0.0628\n",
      "Epoch 52/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9340 - loss: 0.1464 - val_accuracy: 0.9825 - val_loss: 0.0629\n",
      "Epoch 53/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9558 - loss: 0.1159 - val_accuracy: 1.0000 - val_loss: 0.0483\n",
      "Epoch 54/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9349 - loss: 0.1420 - val_accuracy: 1.0000 - val_loss: 0.0499\n",
      "Epoch 55/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9532 - loss: 0.1197 - val_accuracy: 1.0000 - val_loss: 0.0477\n",
      "Epoch 56/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9367 - loss: 0.1439 - val_accuracy: 0.9825 - val_loss: 0.0444\n",
      "Epoch 57/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9431 - loss: 0.1434 - val_accuracy: 0.9649 - val_loss: 0.0969\n",
      "Epoch 58/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9549 - loss: 0.1201 - val_accuracy: 0.9825 - val_loss: 0.0817\n",
      "Epoch 59/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9379 - loss: 0.1447 - val_accuracy: 0.9825 - val_loss: 0.0636\n",
      "Epoch 60/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9529 - loss: 0.1207 - val_accuracy: 0.9825 - val_loss: 0.0816\n",
      "Epoch 61/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9460 - loss: 0.1377 - val_accuracy: 1.0000 - val_loss: 0.0481\n",
      "Epoch 62/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9298 - loss: 0.1568 - val_accuracy: 0.9474 - val_loss: 0.1116\n",
      "Epoch 63/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9322 - loss: 0.1559 - val_accuracy: 0.9825 - val_loss: 0.0713\n",
      "Epoch 64/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9410 - loss: 0.1339 - val_accuracy: 1.0000 - val_loss: 0.0502\n",
      "Epoch 65/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9434 - loss: 0.1477 - val_accuracy: 0.9825 - val_loss: 0.0603\n",
      "Epoch 66/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9320 - loss: 0.1351 - val_accuracy: 0.9825 - val_loss: 0.0673\n",
      "Epoch 67/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9394 - loss: 0.1281 - val_accuracy: 0.9825 - val_loss: 0.0580\n",
      "Epoch 68/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9497 - loss: 0.1451 - val_accuracy: 0.9825 - val_loss: 0.0488\n",
      "Epoch 69/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9633 - loss: 0.1171 - val_accuracy: 0.9123 - val_loss: 0.1544\n",
      "Epoch 70/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9558 - loss: 0.1258 - val_accuracy: 0.9825 - val_loss: 0.0459\n",
      "Epoch 71/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9592 - loss: 0.1267 - val_accuracy: 0.9825 - val_loss: 0.0491\n",
      "Epoch 72/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9465 - loss: 0.1316 - val_accuracy: 0.9825 - val_loss: 0.0612\n",
      "Epoch 73/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9159 - loss: 0.2099 - val_accuracy: 0.9825 - val_loss: 0.0847\n",
      "Epoch 74/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9428 - loss: 0.1264 - val_accuracy: 0.9825 - val_loss: 0.0602\n",
      "Epoch 75/75\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9394 - loss: 0.1682 - val_accuracy: 0.9825 - val_loss: 0.0631\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9383 - loss: 0.1481 \n",
      "Train score: [0.20056389272212982, 0.9142857193946838]\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9883 - loss: 0.0543 \n",
      "Test score: [0.06308437138795853, 0.9824561476707458]\n"
     ]
    }
   ],
   "source": [
    "# According to the previous problem, rebuild your FNN. \n",
    "\n",
    "final_FNN_cancer = keras_FNN_cancer.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=75)\n",
    "print(\"Train score:\", keras_FNN_cancer.evaluate(X_train, y_train))\n",
    "print(\"Test score:\", keras_FNN_cancer.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your final model on the test set using the model.evaluate() function. Report the accuracy on the test set and reflect \n",
    "# on how well your model generalizes to unseen data. \n",
    "\n",
    "# From the previous evaluation code cell, the accuracy on the test set was 98.25% on correctly classifying the unseen data which is a significant 3.51% increase in model\n",
    "# accuracy compared to that of the original model being trained on 100 epochs. The loss also is minimized to 0.063 compared to\n",
    "# 0.159. By re-training the model on the epochs prior to the overfitting phenomenon, we can see that there is a substantial increase\n",
    "# in accuracy and decrease in loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
